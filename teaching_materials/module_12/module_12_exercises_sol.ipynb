{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Videos and Exercises for Session 12: Model Selection and Cross-Validation\n",
    "\n",
    "In this combined teaching module and exercise set, we will investigate how to optimize the choice of hyperparameters using model validation and cross validation. As an aside, we will see how to build machine learning models using a formalized pipeline from preprocessed (i.e. tidy) data to a model.\n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "1. Model Building with Pipelines\n",
    "2. Model Selection and Validation\n",
    "    - Simple Validation\n",
    "    - Cross Validation\n",
    "    - Tools for Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "First, we need to import our standard stuff. Notice that (similar to last session), we are not interested in seeing the convergence warning in scikit-learn, so we suppress them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Model Building with Pipelines\n",
    "\n",
    "A powerful tool for making and applying models are pipelines, which allows to combine different preprocessing and model procedures into one. This has many advantages, mainly being more safe but also has the added side effect being more code-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('e_98Co2xRuQ', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, load the housing dataset from Ex. 11.2.0 using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.1:** Construct a model building pipeline which: \n",
    "> 1. adds polynomial features of degree 3 without bias;\n",
    "> 2. scales the features to mean zero and unit std. \n",
    "\n",
    "> *Hint:* a modelling pipeline can be constructed with `make_pipeline` from `sklearn.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a993cef564dd7353",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "pipe_prep = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),                           \n",
    "                          StandardScaler())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection and Validation\n",
    "\n",
    "\n",
    "## Simple Validation\n",
    "In machine learning, we have two types of parameters: those that are learned from\n",
    "the training data, for example, the weights in logistic regression, and the parameters\n",
    "of a learning algorithm that are optimized separately. The latter are the tuning\n",
    "parameters, also called *hyperparameters*, of a model. These could for example be the regularization\n",
    "parameter in logistic regression or the depth parameter of a decision tree.\n",
    "\n",
    "Below, we investigate how we can choose optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('jSPGLkxJ0cQ', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we will regard the \"train\" (aka. development, non-test) data for two purposes. \n",
    "- First we are interested in getting a credible measure of models under different hyperparameters to perform a model selection. \n",
    "- Then - with the selected model - we estimate/train it on all the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.1:** Make a for loop with 10 iterations where you:\n",
    "> 1. Split the input data into, train (also know as development) and test where the test sample should be one third. Set a new random state for each iteration of the loop, so each iteration makes a different split.\n",
    "> 2. Further split the training (aka development) data into two even sized bins; the first data is for training models and the other is for validating them. Therefore these data sets are often called training and validation.\n",
    "> 3. Train a linear regression model with sub-training data. Compute the RMSE for out-of-sample predictions for both the test data  and the validation data. Save the RMSE.\n",
    ">\n",
    "> You should now have a 10x2 DataFrame with 10 RMSE from both the test data set and the train data set. Compute descriptive statistics of RMSE for the out-of-sample predictions on test and validation data. Are they simular?    \n",
    ">   They hopefuly are pretty simular. This shows us, that we can split the train data, and use this to fit the model. \n",
    ">\n",
    "> *Hint*: you can reuse any code used to solve e.g. exercises 11.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7f4fc2d7f61542ec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(mse(y_pred, y_true))\n",
    "\n",
    "output = []\n",
    "\n",
    "for random_state in range(10):\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=random_state)    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=random_state)\n",
    "\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    reg.predict(X_test)\n",
    "\n",
    "    output.append([rmse(reg.predict(X_val), y_val),\n",
    "                   rmse(reg.predict(X_test), y_test)])\n",
    "    \n",
    "pd.DataFrame(output, columns=['test', 'validation']).describe()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.2:** Construct a model building pipeline which \n",
    "> 1. adds polynomial features of degree 3 without bias;\n",
    "> 2. scales the features to mean zero and unit std. \n",
    "> 3. estimates a Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b9e732407b6a3bc7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pipe_lasso = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),                           \n",
    "                           StandardScaler(),\n",
    "                           Lasso(random_state=1,))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "  \n",
    "The simple validation procedure that we outlined above has one disadvantage: it only uses parts of the *development* data for validation. In the video below, we present a refined approach that uses all the *development* for validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('-G--B3woZGU', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to optimize over both normal parameters and hyperparameters, we do this using nested loops (two-layered cross validation). In the outer loop, we vary the hyperparameters, and then in the inner loop, we do cross validation for the model with the specific selection of hyperparameters. This way, we can find the model with the lowest mean MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.3:**\n",
    "Run a Lasso regression using the Pipeline from `Ex 12.2.2`. In the outer loop, search through the lambdas specified below. \n",
    "In the inner loop, make *5 fold cross validation* on the selected model and store the average MSE for each fold. Which lambda, from the selection below, gives the lowest test MSE?\n",
    ">  ```python \n",
    "> lambdas =  np.logspace(-4, 4, 12)\n",
    "> ```\n",
    "> *Hint:* `KFold` in `sklearn.model_selection` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDED IN ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for model selection\n",
    "\n",
    "Below we review three useful tools for performing model selection. The first tool, the learning curve, can be used to assess whether there is over- and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('BMR6O9NaYdc', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next tool, the validation curve, helps to make perform automated model selection and to visualize the process of model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('c9NkJC7EzPg', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have more than one hyperparameter, we need to find the combination of optimal hyperparameters. In the video below we see how to do that for *elastic net*, which has both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('AjEpa24mFGw', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.4:** __Automated Cross Validation in one dimension__  \n",
    "Now we want to repeat exercise 12.2.3 in a more automated fashion. \n",
    "When you are doing cross validation with one hyperparameter, you can automate the process by using `validation_curve` from `sklearn.model_selection`. Use this function to search through the values of lambdas, and find the value of lambda, which give the lowest test error.  \n",
    ">\n",
    "> Check if you got the same output for the manual implementation (Ex. 12.2.3) and the automated implementation (Ex. 12.2.4) \n",
    ">\n",
    "> BONUS: Plot the average MSE-test and MSE-train against the different values of lambda. (*Hint*: Use logarithmic axes, and lambda as index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0de0809041fc3c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator=pipe_lasso,\n",
    "                     X=X_dev,\n",
    "                     y=y_dev,\n",
    "                     param_name='lasso__alpha',\n",
    "                     param_range=lambdas,\n",
    "                     scoring='neg_mean_squared_error',# scoring='neg_mean_squared_error',                 \n",
    "                     cv=5)\n",
    "\n",
    "mean_values = pd.concat({'train': pd.DataFrame(-train_scores).mean(1), \n",
    "                         'test': pd.DataFrame(-test_scores).mean(1), \n",
    "                         'lambda': pd.DataFrame(lambdas).mean(1)}, axis =1)\n",
    "\n",
    "# answer to plotting bonus question\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(mse(y_pred, y_true))\n",
    "\n",
    "\n",
    "# plot curves\n",
    "pd.concat({'train': pd.DataFrame(-train_scores).mean(1), \n",
    "           'test': pd.DataFrame(-test_scores).mean(1)},\n",
    "           axis=1)\\\n",
    "    .pipe(np.sqrt)\\\n",
    "    .set_index(pd.Index(lambdas, name='lambda'))\\\n",
    "    .plot(logx=True, logy=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have *more than one* hyperparameter, you will want to fit the model to all the possible combinations of hyperparameters. This is done in an approch called `Grid Search`, which is implementet in `sklearn.model_selection` as `GridSearchCV`\n",
    "\n",
    "> **Ex. 12.2.5:** To get to know `Grid Search`, we want to implement it in one dimension. Using `GridSearchCV`, implement the Lasso, with the same lambdas as before (`lambdas =  np.logspace(-4, 4, 12)`), 10-fold CV and (negative) mean squared error as the scoring variable. Which value of Lambda gives the lowest test error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c3aad24ac0d4f7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_lasso, \n",
    "                  param_grid=[{'lasso__alpha':lambdas}], \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  cv=10, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_dev, y_dev)\n",
    "print(gs.best_params_)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.6 BONUS** Now set `lambdas =  np.logspace(-4, 4, 100)`, and repeat the previous now with RandomizedSearchCV with `n_iter=12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "lambdas_new = np.logspace(-4, 4, 100)\n",
    "\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator = pipe_lasso,\n",
    "    param_distributions = [{\"lasso__alpha\": lambdas_new}], \n",
    "    cv = 10, \n",
    "    scoring = \"neg_mean_squared_error\",\n",
    "    n_iter = 12)\n",
    "\n",
    "gs.fit(X_dev,y_dev)\n",
    "\n",
    "print(gs.best_params_)\n",
    "\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
